{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QACY3c_lr4uz"
      },
      "source": [
        "# Lab04: Wiki Indexer.\n",
        "\n",
        "- MSSV: 19120689\n",
        "- Họ và tên: Lại Khánh Toàn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD8WblEMr4u1"
      },
      "source": [
        "## Yêu cầu bài tập\n",
        "\n",
        "**Cách làm bài**\n",
        "\n",
        "\n",
        "Bạn sẽ làm trực tiếp trên file notebook này; từ `TODO` cho biết những phần mà bạn cần phải làm.\n",
        "\n",
        "Bạn có thể thảo luận ý tưởng cũng như tham khảo các tài liệu, nhưng *code và bài làm phải là của bạn*. \n",
        "\n",
        "Nếu vi phạm thì sẽ bị 0 điểm cho bài tập này.\n",
        "\n",
        "**Cách nộp bài**\n",
        "\n",
        "Trước khi nộp bài, rerun lại notebook (`Kernel` -> `Restart & Run All`).\n",
        "\n",
        "Sau đó, tạo thư mục có tên `MSSV` của bạn (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`) Chép file notebook, file `data.json` và file `inverted_index.json` vào thư mục của các bạn (nếu file này kích thước lớn các bạn có thể chép link vào `link_data.txt`), nén thư mục `MSSV` này lại và nộp trên moodle.\n",
        "\n",
        "**Nội dung bài tập**\n",
        "Dựa trên dữ liệu từ wiki, xây dựng inverted_index và tiến hành tìm kiếm dữ liệu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txFiZE0Zr4u2"
      },
      "source": [
        "## Nội dung bài tập"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDREKsiC5K6Q"
      },
      "source": [
        "### Thư viện mediawiki\n",
        "\n",
        "- mediawiki is a python wrapper and parser for the MediaWiki API. The goal is to allow users to quickly and efficiently pull data from the MediaWiki site of their choice instead of worrying about dealing directly with the API\n",
        "- For more information: https://pymediawiki.readthedocs.io/en/latest/quickstart.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Su_weRPD5S85",
        "outputId": "c9109418-6713-4e01-8aa9-aa30c5e8c6d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymediawiki\n",
            "  Downloading pymediawiki-0.7.2-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pymediawiki) (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from pymediawiki) (4.11.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->pymediawiki) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->pymediawiki) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->pymediawiki) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->pymediawiki) (3.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->pymediawiki) (2.4.1)\n",
            "Installing collected packages: pymediawiki\n",
            "Successfully installed pymediawiki-0.7.2\n"
          ]
        }
      ],
      "source": [
        "pip install pymediawiki"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7Ivi0x_5K6R",
        "outputId": "efbaea57-9ac2-471f-b5eb-6bb9b1327a78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Guido Barbujani (born January 31, 1955) is an Italian population geneticist, evolutionary biologist and literary author born in Adria, who has worked with the State University of New York at Stony Brook (NY), University of Padua, and University of Bologna. He has taught at the University of Ferrara since 1996.\n",
            "\n",
            "\n",
            "== Works ==\n",
            "A population geneticist by training, Barbujani has been working on several aspects of human genetic variation. In collaboration with Robert R. Sokal, he pioneered the statistical comparison of patterns of genetic and linguistic variation, showing that language differences may contribute to reproductive isolation, and hence promote genetic divergence between populations.\n",
            "His analyses of geographic patterns of genetic variation in Europe support Luca Cavalli-Sforza's Neolithic demic diffusion model, or the idea that farming spread in the Neolithic mainly because farmers did, and not by cultural transmission. There are two implications of this finding: first, that most Europeans' ancestors, up to Neolithic times, did not live in geographical Europe, but rather in the Near East; and second, that the early farmers expanding west carried with them their genes, their technologies, and possibly their languages.\n",
            "His studies of the amount of DNA differentiation among human populations, and of its spatial distribution, led to the conclusion that traditional human racial classifications fail to account for most of the existing patterns of genetic variation. Rather, it seems that genetic variation is largely uncorrelated across genes, which, if confirmed, would explain why no consensus was ever reached on a catalog of human biological races. This activity has also resulted in publications for the general public.\n",
            "His recent DNA studies focus on genetic characterization of ancient human populations, such as Paleolithic anatomically modern humans of Cro-Magnoid morphology, and groups like the Etruscans and the Sardinians from the Nuragic era in the Neolithic.Barbujani is the author of three novels.\n",
            "\n",
            "\n",
            "== Quote ==\n",
            "\"The idea that all humans naturally belong to one of a few biological types or races that evolved in isolation was unchallenged for centuries, but large-scale modern studies failed to associate racial labels with recognizable genetic clusters.\" (Barbujani G., 2005, p. 215)\n",
            "\n",
            "\n",
            "== References ==\n",
            "\n",
            "\n",
            "== Scientific Bibliography ==\n",
            "Barbujani G. and Sokal R.R. (1990) Zones of sharp genetic change in Europe are also linguistic boundaries.  Proceedings of the National Academy of Sciences USA 87:1816-1819.\n",
            "Barbujani G., Magagni A., Minch E. and Cavalli-Sforza L.L. (1997) An apportionment of human DNA diversity.  Proceedings of the National Academy of Sciences USA 94:4516-4519.\n",
            "Barbujani G. and Bertorelle G. (2001) Genetics and the population history of Europe.  Proceedings of the National Academy of Sciences USA 98:22-25.\n",
            "Chikhi L., Destro-Bisol G., Bertorelle G., Pascali V., and Barbujani G.  (1998) Clines of nuclear DNA markers suggest a recent, Neolithic ancestry of the European gene pool. Proceedings of the National Academy of Sciences USA, 95:9053-9058.\n",
            "Romualdi C., Balding D., Nasidze I.S., Risch G., Robichaux M., Sherry S., Stoneking  M., Batzer M. and Barbujani G. (2002) Patterns of human diversity, within and among continents, inferred from biallelic DNA polymorphisms. Genome Research 12:602-612.\n",
            "Barbujani G. and Goldstein D.B. (2004) Africans and Asians abroad: Genetic diversity in Europe. Annual Review of Genomics and Human Genetics 5:119-150.\n",
            "Dupanloup I., Bertorelle G., Chikhi L. and Barbujani G. (2004) Estimating the impact of prehistoric admixture on the Europeans’ genome. Molecular Biology and Evolution 21:1361-1372\n",
            "Barbujani G. (2005) Human races: Classifying people vs. understanding diversity. Current Genomics 6:215-226\n",
            "Belle E.M.S., Ramakrishnan U., Mountain J. and Barbujani G. (2006) Serial coalescent simulations suggest a weak genealogical relationship between Etruscans and modern Tuscans. Proceedings of the National Academy of Sciences USA 103:8012-8017.\n",
            "Caramelli D., Milani L., Vai S., Modi A., Pecchioli E., Girardi M., Pilli E., Lari M., Lippi B., Ronchitelli A., Mallegni F., Casoli A., Bertorelle G., Barbujani G. (2008) A 28,000 years old Cro–Magnon mtDNA sequence differs from all potentially contaminating modern sequences. PLoS ONE 3:e2700.\n",
            "Ghirotto S., Mona S., Benazzo A., Paparazzo F., Caramelli D., Barbujani G. (2010) Inferring genealogical processes from patterns of Bronze–age and modern DNA variation in Sardinia. Molecular Biology and Evolution 27:775–786.\n",
            "Barbujani G. and Colonna V. (2010) Human genome diversity: Frequently asked questions. Trends in Genetics 26:285–295.\n",
            "\n",
            "\n",
            "== Bibliography (Books, Nonfiction) ==\n",
            "Barbujani, Guido (2006). L'invenzione delle razze. Bompiani, Milan. Portuguese translation: A invencão das racas (2007). São Paulo, Brazil: Editora Contexto.\n",
            "Barbujani, Guido; Cheli, P. (2008). Sono razzista, ma sto cercando di smettere. Rome-Bari: Laterza.\n",
            "Barbujani, Guido (2008). Europei senza se e senza ma. Storie di neandertaliani e di immigrati. Milan: Bompiani.\n",
            "Barbujani, Guido (2016). Gli africani siamo noi. Alle origini dell'uomo. Bari: Laterza.\n",
            "\n",
            "\n",
            "== Bibliography (Books, Fiction) ==\n",
            "Dilettanti. Marsilio, Venice, 1994 (Republished as: Dilettanti. Quattro viaggi nei dintorni di Charles Darwin. Sironi, Milan, 2004)\n",
            "Dopoguerra. Sironi, Milan, 2002.\n",
            "Questione di razza. Mondadori, Milan, 2003.\n",
            "\n",
            "\n",
            "== External links ==\n",
            "https://web.archive.org/web/20080731132037/http://web.unife.it/progetti/genetica/Guido/ : Personal webpage, with access to pdfs of scientific articles.\n",
            "https://www.sciencedaily.com/releases/2006/05/060526065706.htm : ancient Etruscans unlikely ancestors of modern Tuscans. Science Daily, April 2006.\n",
            "https://web.archive.org/web/20081217011325/http://www.festivaldellamente.it/pdf/ENG_2007_programme.pdf : taped interview on human diversity (in Italian), October 2007.\n"
          ]
        }
      ],
      "source": [
        "from mediawiki import MediaWiki\n",
        "import json\n",
        "\n",
        "wikipedia = MediaWiki(lang='en')\n",
        "\n",
        "rp = wikipedia.random(pages=2)\n",
        "p = wikipedia.page(rp[0])\n",
        "print(p.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feKRlSvi5K6S"
      },
      "source": [
        "# Lấy dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xLmpse6T5K6S"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "#Lấy dữ liệu từ wiki\n",
        "def getWikiContent(num_page):\n",
        "    # Dữ liệu sẽ được lưu theo dạng\n",
        "    # [\n",
        "    #     {\n",
        "    #         \"id\": ....,\n",
        "    #         \"url\": ....,\n",
        "    #         \"title\": ...,\n",
        "    #         \"content\": ...,\n",
        "    #     },\n",
        "    #     {\n",
        "    #         \"id\": ....,\n",
        "    #         \"url\": ....,\n",
        "    #         \"title\": ...,\n",
        "    #         \"content\": ...,\n",
        "    #     }\n",
        "    #     ...\n",
        "    # ]\n",
        "    # id: được đánh số từ 1->5000 \n",
        "    data = []\n",
        "\n",
        "    for i in range(num_page):\n",
        "        rp = wikipedia.random(pages=1)\n",
        "        p = wikipedia.page(rp[0])\n",
        "        data.append({\n",
        "            \"id\": i+1,\n",
        "            \"url\": p.url,\n",
        "            \"title\": p.title,\n",
        "            \"content\": p.content\n",
        "        })\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EzMvdynd5K6S"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "# Lưu dữ liệu vào file 'data.json'\n",
        "def saveWikiContent(data):\n",
        "    with open('data.json', 'w') as f:\n",
        "        json.dump(data, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NdnUVCZ35K6S"
      },
      "outputs": [],
      "source": [
        "#Sử dụng dữ liệu được lấy từ 5000 trang wikipedia ngẫu nhiên\n",
        "# num_page = 5000\n",
        "num_page = 2500\n",
        "wiki_content = getWikiContent(num_page)\n",
        "saveWikiContent(wiki_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XRwqWv05K6T"
      },
      "source": [
        "# Load dữ liệu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Q8MGqyRJ5K6T"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "#Load dữ liệu từ file dữ liệu đã lưu\n",
        "def loadData(file_name):\n",
        "    with open(file_name, 'r') as f:\n",
        "      data = json.load(f)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fZj4GvOz5K6T"
      },
      "outputs": [],
      "source": [
        "file_name = \"data.json\"\n",
        "wiki_content = loadData(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chkEuAan5K6T"
      },
      "source": [
        "# Inverted-Index\n",
        "Inverted index is a data structure used in information retrieval systems to efficiently retrieve documents or web pages containing a specific term or set of terms. In an inverted index, the index is organized by terms (words), and each term points to a list of documents or web pages that contain that term.\n",
        "\n",
        "For example, consider the following documents:\n",
        "\n",
        "    Document 1: The quick brown fox jumped over the lazy dog.\n",
        "    Document 2: The lazy dog slept in the sun.\n",
        "\n",
        "To create an inverted index for these documents, we first tokenize the documents into terms, as follows:\n",
        "\n",
        "    Document 1: The, quick, brown, fox, jumped, over, the, lazy, dog.\n",
        "    Document 2: The, lazy, dog, slept, in, the, sun.\n",
        "\n",
        "Next, we create an index of the terms, where each term points to a list of documents that contain that term, as follows:\n",
        "\n",
        "    The -> Document 1, Document 2\n",
        "    quick -> Document 1\n",
        "    brown -> Document 1\n",
        "    fox -> Document 1\n",
        "    jumped -> Document 1\n",
        "    over -> Document 1\n",
        "    lazy -> Document 1, Document 2\n",
        "    dog -> Document 1, Document 2\n",
        "    slept -> Document 2\n",
        "    in -> Document 2\n",
        "    sun -> Document 2\n",
        "\n",
        "To search for documents containing a particular term or set of terms, the search engine queries the inverted index for those terms and retrieves the list of documents associated with each term. The search engine can then use this information to rank the documents based on relevance to the query and present them to the user in order of importance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdvkPQQZ4x0j",
        "outputId": "d8118d95-6a61-46de-fb50-50d5a7a0cc0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "english_stopwords = stopwords.words('english')\n",
        "# print(english_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "suZmNgpQ5K6T"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "#Xử lý dữ liệu\n",
        "def processData(data):\n",
        "    #trong mỗi trường dữ liệu title, content thực hiện việc xử lý dữ liệu\n",
        "\n",
        "    # loại bỏ kí tự đặc biệt\n",
        "    # tách từ\n",
        "\n",
        "    # chuyển đổi dữ liệu về chữ thường\n",
        "\n",
        "    # Loại bỏ các từ mà nằm trong danh sách english_stopwords\n",
        "\n",
        "    # Loại bỏ các từ không phải tiếng anh\n",
        "\n",
        "    # chuyển từ về dạng nguyên gốc (stemming)\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    for item in data:\n",
        "      for field in ['title', 'content']:\n",
        "        text = item[field]\n",
        "\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        words = word_tokenize(text)\n",
        "        words = [word.lower() for word in words]\n",
        "        words = [word for word in words if word not in english_stopwords]\n",
        "        words = [word for word in words if word.isascii()]\n",
        "        words = [ps.stem(word) for word in words]\n",
        "        item[field] = ' '.join(words)\n",
        "\n",
        "    return data  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6TkVEbeM5K6T"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "#Tính toán inverted index\n",
        "def invertedIndex(data):\n",
        "    # thực hiện việc tính inverted index cho riêng title, content\n",
        "    # lưu inverted_index dưới dạng\n",
        "    # {\n",
        "    #     \"word1\": {\n",
        "    #         \"t\": [id1, id2,...], (inverted index cho title)\n",
        "    #         \"c\" : [id2, id4,...], (inverted index cho content)\n",
        "    #     },\n",
        "    #     \"word2\": {\n",
        "    #         \"t\": [id1, id2,...], (inverted index cho title)\n",
        "    #         \"c\" : [id2, id4,...], (inverted index cho content)\n",
        "    #     },\n",
        "    # }\n",
        "\n",
        "    inverted_index = {}\n",
        "    \n",
        "    for item in data:\n",
        "        id = item['id']\n",
        "        \n",
        "        for field in ['title', 'content']:\n",
        "            words = item[field].split()\n",
        "            \n",
        "            for word in words:\n",
        "                if word not in inverted_index:\n",
        "                    inverted_index[word] = {'t': [], 'c': []}\n",
        "                \n",
        "                if id not in inverted_index[word][field[0]]:\n",
        "                    inverted_index[word][field[0]].append(id)\n",
        "\n",
        "    return inverted_index\n",
        "\n",
        "#TODO\n",
        "#Lưu dữ liệu inverted index\n",
        "def saveFileIndex(data):\n",
        "    #Lưu inverted_index vào file \"inverted_index.json\"\n",
        "    with open('inverted_index.json', 'w') as f:\n",
        "      json.dump(data, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kyW5Zqe_5K6U"
      },
      "outputs": [],
      "source": [
        "data = processData(wiki_content)\n",
        "\n",
        "inverted_index = invertedIndex(data)\n",
        "\n",
        "saveFileIndex(inverted_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6yB6rD25K6U"
      },
      "source": [
        "# Tìm kiếm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2IG8Eorl5K6U"
      },
      "outputs": [],
      "source": [
        "inverted_index = loadData(\"inverted_index.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ELUe727M1R-H"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "# Tìm kiếm dữ liệu có chứa query\n",
        "# Trả về id của các bộ document phù hợp \n",
        "def search(query):\n",
        "    \n",
        "    # xử lý câu query (tương tự xử lý dữ liệu wiki)\n",
        "\n",
        "    # Tìm kiếm dữ liệu có chứa từ trong query\n",
        "\n",
        "    # Thứ tự tìm kiếm ưu tiên:  ưu tiên tìm xem có document nào chứa tất cả từ trong query không, \n",
        "    # nếu không có thì bắt đầu tìm document chứa một vài từ có trong query\n",
        "    \n",
        "    # Độ ưu tiên khi thực hiện tìm kiếm dữ liệu: ưu tiên tìm kiếm query nằm trong title trước, sau đó tìm kiếm query nằm trong content. \n",
        "    # Nếu document đó đã xuất hiện query trong title rồi thì không tìm kiếm trong content nữa\n",
        "    \n",
        "    # Danh sách kết quả các id của document tìm được được sắp xếp theo: \n",
        "    # Các document có query xuất hiện trong title(từ có các document chứa nhiều từ query nhất đến ít từ query xuất hiện nhất)\n",
        "    # Các document có query xuất hiện trong content(từ có các document chứa nhiều từ query nhất đến ít từ query xuất hiện nhất)\n",
        "\n",
        "    ps = PorterStemmer()\n",
        "    # Xử lý query \n",
        "    query = re.sub(r'[^\\w\\s]', '', query)\n",
        "    words = word_tokenize(query)\n",
        "    words = [word.lower() for word in words]\n",
        "    words = [word for word in words if word not in english_stopwords]\n",
        "    words = [ps.stem(word) for word in words]\n",
        "    \n",
        "    # Search documents chứa tất cả từ trong query\n",
        "    title_results = []\n",
        "    content_results = []\n",
        "    \n",
        "    for word in words:\n",
        "        if word in inverted_index:\n",
        "            title_results.append(set(inverted_index[word]['t']))\n",
        "            content_results.append(set(inverted_index[word]['c']))\n",
        "    \n",
        "    title_results = set.intersection(*title_results)\n",
        "    content_results = set.intersection(*content_results)\n",
        "    \n",
        "    # Search documents chứa 1 vài từ trong query\n",
        "    if not title_results and not content_results:\n",
        "        title_results = []\n",
        "        content_results = []\n",
        "        \n",
        "        for word in words:\n",
        "            if word in inverted_index:\n",
        "                title_results.extend(inverted_index[word]['t'])\n",
        "                content_results.extend(inverted_index[word]['c'])\n",
        "        \n",
        "        title_results = set(title_results)\n",
        "        content_results = set(content_results)\n",
        "    \n",
        "    # Loại bỏ documents của content đã có trong title\n",
        "    content_results -= title_results\n",
        "\n",
        "    title_results = list(set(title_results))\n",
        "    \n",
        "    content_results = list(set(content_results))\n",
        "\n",
        "    # Sắp xếp kết quả theo số từ query xuất hiện trong title\n",
        "    title_results.sort(key=lambda x: sum([1 for w in words if w in data[x]['title']]), reverse=True)\n",
        "\n",
        "    # Sắp xếp kết quả theo số từ query xuất hiện trong content \n",
        "    content_results.sort(key=lambda x: sum([1 for w in words if w in data[x]['content']]), reverse=True)\n",
        "    \n",
        "    return title_results + content_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nnlR6aDw2Na",
        "outputId": "012cc1b2-70fd-4d34-8181-d4255c085c7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2119,\n",
              " 72,\n",
              " 1608,\n",
              " 1272,\n",
              " 259,\n",
              " 2312,\n",
              " 1293,\n",
              " 1322,\n",
              " 355,\n",
              " 2404,\n",
              " 1476,\n",
              " 520,\n",
              " 1544,\n",
              " 1037,\n",
              " 1549,\n",
              " 15,\n",
              " 2065,\n",
              " 2070,\n",
              " 1047,\n",
              " 1052,\n",
              " 32,\n",
              " 2081,\n",
              " 1058,\n",
              " 2084,\n",
              " 1588,\n",
              " 1082,\n",
              " 571,\n",
              " 1093,\n",
              " 2120,\n",
              " 79,\n",
              " 88,\n",
              " 1119,\n",
              " 1639,\n",
              " 1641,\n",
              " 1138,\n",
              " 629,\n",
              " 1141,\n",
              " 634,\n",
              " 1147,\n",
              " 2171,\n",
              " 125,\n",
              " 1664,\n",
              " 642,\n",
              " 1154,\n",
              " 645,\n",
              " 2182,\n",
              " 1682,\n",
              " 666,\n",
              " 1690,\n",
              " 163,\n",
              " 2211,\n",
              " 165,\n",
              " 677,\n",
              " 680,\n",
              " 1195,\n",
              " 175,\n",
              " 1201,\n",
              " 1719,\n",
              " 1217,\n",
              " 194,\n",
              " 718,\n",
              " 2265,\n",
              " 1760,\n",
              " 1763,\n",
              " 1253,\n",
              " 1766,\n",
              " 2279,\n",
              " 1257,\n",
              " 1770,\n",
              " 2283,\n",
              " 750,\n",
              " 2291,\n",
              " 760,\n",
              " 1273,\n",
              " 2296,\n",
              " 1276,\n",
              " 1788,\n",
              " 2298,\n",
              " 260,\n",
              " 1796,\n",
              " 2313,\n",
              " 1804,\n",
              " 1294,\n",
              " 1812,\n",
              " 286,\n",
              " 1310,\n",
              " 288,\n",
              " 1827,\n",
              " 2343,\n",
              " 298,\n",
              " 810,\n",
              " 301,\n",
              " 1323,\n",
              " 2347,\n",
              " 2350,\n",
              " 305,\n",
              " 2352,\n",
              " 1334,\n",
              " 1339,\n",
              " 1855,\n",
              " 324,\n",
              " 2374,\n",
              " 328,\n",
              " 1870,\n",
              " 335,\n",
              " 2386,\n",
              " 858,\n",
              " 860,\n",
              " 1375,\n",
              " 867,\n",
              " 2405,\n",
              " 1391,\n",
              " 2420,\n",
              " 373,\n",
              " 1912,\n",
              " 1414,\n",
              " 1929,\n",
              " 1418,\n",
              " 2445,\n",
              " 408,\n",
              " 2463,\n",
              " 1455,\n",
              " 1461,\n",
              " 1474,\n",
              " 1477,\n",
              " 1996,\n",
              " 1495,\n",
              " 1501,\n",
              " 1509,\n",
              " 2024,\n",
              " 490,\n",
              " 1514,\n",
              " 1516,\n",
              " 496,\n",
              " 1521,\n",
              " 1523,\n",
              " 2038,\n",
              " 1527,\n",
              " 508]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "search('web mining')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0WUJrUDF5K6U"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "#Đánh giá thời gian thực hiện với các câu query từ 1 đến 5 từ => Đưa ra nhận xét\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "m4hXfwEWzYMZ"
      },
      "outputs": [],
      "source": [
        "time_results = []\n",
        "\n",
        "def time_calculate(test_cases):\n",
        "  res = dict()\n",
        "\n",
        "  for val in test_cases:\n",
        "    start = time.time()\n",
        "    search(val)\n",
        "    end = time.time()\n",
        "\n",
        "    res[val] = end - start\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "NB5YzXhFAxhb"
      },
      "outputs": [],
      "source": [
        "case_1 = ['web mining', 'data science and data engineer', 'computer science', 'mobile web technology', 'game develoment with python']\n",
        "\n",
        "case_2 = ['apple', 'apple iphone', 'machine learning algorithms for image classification', 'running shoes brands']\n",
        "\n",
        "r1 = time_calculate(case_1)\n",
        "\n",
        "r2 = time_calculate(case_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGN0H6AMHlIK",
        "outputId": "df5de197-e0e8-4269-f955-82fbfd3c3324"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'web mining': 0.002887248992919922,\n",
              " 'data science and data engineer': 0.018986225128173828,\n",
              " 'computer science': 0.006317853927612305,\n",
              " 'mobile web technology': 0.021956682205200195,\n",
              " 'game develoment with python': 0.013913869857788086}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "r1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9pz_NaoHzgr",
        "outputId": "f1cd6178-1230-45c9-bdeb-afd3a8ad3720"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'apple': 0.02205181121826172,\n",
              " 'apple iphone': 0.0014226436614990234,\n",
              " 'machine learning algorithms for image classification': 0.03355884552001953,\n",
              " 'running shoes brands': 0.00941014289855957}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "r2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Nhận xét:\n",
        "- Thời gian tìm kiếm dữ liệu phụ thuộc vào độ dài query, query càng dài thì càng tốn nhiều thời gian thực thi hơn (làm sạch query, tìm dữ liệu khớp với từ trong query).\n",
        "- Bên cạnh đó thời gian còn phụ thuộc vào kích thước file inverted_index, file càng lớn thì thời gian tìm kiếm càng lâu vì cần phải duyệt qua nhiều từ của query với nhiều từ trong file inverted_index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
